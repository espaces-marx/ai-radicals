## Quel impact global de l'IA ?

_**Comment poser une question à ChatGPT en France peut produire des émissions de CO2 et une consommation d'eau potable aux Etats-Unis ?**_ 
Pour bien répondre à cette question et mieux mesurer comment maîtriser nos usages individuels et collectifs, il est utile de comprendre comment fonctionne l'IA générative.

**Pour générer un message, l'intelligence artificielle fait une prédiction statistique à partir de sa compréhension de notre langue.** Pour un outil comme ChatGPT cette connaissance est issue d'un entraînement intensif, où le programme doit deviner mot par mot le contenu d'un texte dont seul le premier mot n'est pas masqué.

À ce stade, la future IA générative est une forme bien plus évoluée -_mais pas entièrement différente_- de l'outil d'auto-complétion que l'on retrouve dans nos smartphones lorsque l'on écrit un message ; ce dernier nous propose ce qui lui paraît être le prochain mot le plus probable.
Ces algorithmes plus primitifs mémorisent nos choix de mots à partir des messages que nous écrivons, et ils gagnent une efficacité -_relative_- au fur et à mesure que la quantité de nos messages écrits augmente. Cependant ils ont une représentation très pauvre de notre langue, qui se limite à enregistrer combien de fois le mot "Je", par exemple, a été immédiatement suivi de "suis". 

**La compréhension des relations entre les mots est bien plus complexe dans un modèle du même type que ChatGPT, dit _Transformer_[^8].** L'idée derrière le fonctionnement d'un Transformer est formalisée en 2017, dans un article de recherche historique dans le domaine de l'apprentissage automatique, "_Attention is all you need_"[^7] ("_L'attention est tout ce dont vous avez besoin_") produit par une équipe de chercheurs de Google. 
**Comment fonctionnent ces modèles ?** Au lieu de se concentrer uniquement sur la fréquence à laquelle un mot en suit un autre, **les Transformers s'appuient sur des techniques développées à l'origine pour des outils de traduction**. Chaque langue ayant des règles différentes pour l'ordre d'une phrase ou la conjugaison des mots, les outils de traduction ont besoin de pouvoir **porter leur attention sur différentes parties d'une phrase** en langue d'origine -_pas uniquement le mot précédant celui à traduire_-, pour en produire une version traduite dans la langue ciblée. Par exemple pour traduire en français le mot anglais "_this_" (ce / cette, en français), il faut pouvoir identifier dans l'ensemble d'une phrase quel est le sujet et déterminer si celui-ci est masculin ou féminin ("_ce tabouret_" ou "_cette table_"). 

**C'est une forme avancée de cette capacité à cibler l'attention du programme sur certaines parties d'une phrase, qui est au cœur du fonctionnement de l'IA générative**. Lors de son entraînement, le programme va identifier de façon autonome (sans qu'un humain "étiquette" les données) des motifs statistiques récurrents dans les textes utilisés (par exemple, les accords sujets-verbe ou l'utilisation de noms propres) et spécialiser différentes "_couches d'attention_" pour les reconnaître et améliorer ses prédictions. Cette compréhension mathématique du langage sera conservée dans les paramètres du modèle d'IA, dont le nombre va de quelques milliards à plus d'un trillion.

L'entraînement lui-même est fait sur un corpus de textes bien plus important que nos messages personnels. Pour les plus petits modèles (moins de 10 milliards de paramètres), la quantité de texte utilisée est équivalente à entre **1000 et 10 000 fois l'ensemble de la production écrite de Shakespeare, où 1 à 3 fois l'ensemble des pages Wikipédia écrites en français** (3 à 4 milliards de mots). 
Pour les modèles de moyenne à grande taille (de plusieurs dizaines à plusieurs centaines de milliards de paramètres)  cela pourrait aller de l'équivalent de **l'ensemble des documents textuels de la Bibliothèque Nationale de France** (40 millions de documents et 15 millions de livres) s'ils étaient tous numérisés, à **10 fois l'ensemble des articles de Wikipédia**, toutes langues confondues. 
La limite haute de la quantité de données utilisées pour l'entraînement des plus grands modèles d'IA augmente considérablement chaque année, tant qu'elle pourrait croiser entre 2026 et 2032 la courbe de la quantité de textes exploitables disponibles : c'est à dire de l'ensemble des textes numériques, d'une qualité suffisante pour être exploités, et publiquement accessibles ([ref](https://epoch.ai/blog/will-we-run-out-of-data-limits-of-llm-scaling-based-on-human-generated-data)). L'entraînement des modèles ne serait alors plus limité par des facteurs techniques, mais par une quantité insuffisante de textes -_numérisés_- produits par l'humanité dans son ensemble.

### Intelligence artificielle et centres de données

**Entraîner un modèle d'IA sur autant de données sans y dédier des années demande une capacité de calcul phénoménale**, bien au delà de celle qui peut être atteinte par nos ordinateurs personnels. Comme pour la quantité de textes utilisée, la puissance de calcul mobilisée pour l'entraînement des plus grands modèles connaît une croissance explosive; au point qu'on estime qu'elle double tous les 6 mois ([ref](https://epoch.ai/data/ai-models)). 

Pour atteindre leurs objectifs d'entraînement, les entreprises développant de nouveaux modèles d'IA ont donc recours à des centres de données, qui regroupent en un même lieu des ordinateurs en réseau avec une grande puissance de calcul et une capacité importante de stockage (utile pour la quantité de données servant à l'entraînement d'une IA). 

>[!info] **Les centres de données : le "_Cloud_" au sol**
L'utilisation des centres de données n'est ni nouvelle, ni une invention due à l'intelligence artificielle. Ils sont la forme matérielle derrière le concept de "_Cloud (computing)_". 
Concrètement, un centre de données permet un gain d'efficacité par rapport à une situation ou ne seraient exploités que le stockage et les puissances de calcul individuelles de nos ordinateurs personnels, ou de quelques serveurs isolés dans chaque entreprise. 

La plupart d'entre nous utilisons au quotidien un ou des services qui emploient des centres de données :  recherches et suite Google (Docs, Drive, Gmail, ...), réseaux sociaux, Netflix et ses concurrents, Spotify; sans parler des simples sites avec un hébergement partagé. Une certaine mesure de l'efficacité de ce système est qu'entre 2010 et 2017, malgré l'augmentation continue de ce type de services (développement des réseaux sociaux, streaming), la dépense en électricité liée aux centres de données est restée globalement stable aux Etats-Unis, en dessous de 2% de l'électricité consommée à l'échelle du pays ([ref](https://escholarship.org/uc/item/32d6m0d1)).

**S'ils permettent une meilleure efficacité de la dépense énergétique, les centres de données peuvent également avoir un impact environnemental important.**  Pour fonctionner correctement, ils ont besoin d'un approvisionnement constant en énergie, sans aucune interruption possible. Peut-être pour cette raison, les quelques 3000 ([ref](https://www.datacentermap.com/)) centres de données aux Etats-Unis ont tendance à se regrouper dans des zones avec des réseaux d'électricité incluant moins d'énergies renouvelables variables (éolien, solaire) et consomment une énergie en moyenne 48% plus émettrice de CO2([ref](https://www.technologyreview.com/2024/12/13/1108719/ais-emissions-are-about-to-skyrocket-even-further/)) que l'électricité générale américaine (elle-même déjà 9 fois plus carbonée que l'énergie française[^4]). 

La quantité d'émissions de CO2 liée à l'alimentation des centres de données en électricité varie grandement selon les pays, mais les Etats-Unis ont une importance particulière dans ce domaine. Pas moins de 45% de l'utilisation globale d'électricité pour les centres de données s'y concentre, contre seulement 25% pour la Chine, 15% pour l'Europe dans son ensemble et 15% également pour le reste du monde ([ref](https://www.iea.org/reports/energy-and-ai/executive-summary)). Avec la faible part européenne dans ce total, on comprend que les émissions de CO2 liées au numérique en France ont une grande dépendance vis à vis des politiques énergétiques étrangères -_particulièrement celles des Etats-Unis_-, dans tous nos usages susceptibles d'utiliser des centres de données. 

On le ressent notamment dans l'évolution des chiffres de l'ADEME (Agence de l'Environnement et de la Maîtrise de l'Énergie) qui évaluait à 2,5% l'impact du numérique en 2020 sur l'empreinte carbone de la France[^5], avec une contribution modeste (16%) des centres de données - _français_ - à ce chiffre. Pour 2022, l'ADEME a produit une nouvelle estimation presque doublée, de 4,4% de poids du numérique dans l'empreinte carbone nationale, en prenant cette fois en compte les émissions liées à des utilisations françaises de centres de données étrangers, contribuant  - _avec les centres français_ - à 46% de l'empreinte carbone du numérique ([ref](https://infos.ademe.fr/magazine-janvier-2025/numerique-quel-impact-environnemental-en-2022/)). 

**Quelle part de l'IA dans l'impact environnemental des centres de données ?**
Le développement de l'intelligence artificielle selon le modèle que l'on a décrit - _des Transformers_ - depuis 2017 semble avoir produit une transformation profonde des centres de données, dont la dynamique se poursuit.
On peut notamment l'observer à travers l'évolution de leur consommation d'électricité aux Etats-Unis. Après être restée stable entre 2010 et fin 2016, elle a connu une forte croissance pour atteindre en 2023 le triple de son niveau pré-IA - _celui de 2016_ - et est projeté en 2028 entre 6 et 9 fois ce même niveau ([ref](https://www.energy.gov/articles/doe-releases-new-report-evaluating-increase-electricity-demand-data-centers)). 

**Pourquoi une telle évolution ?** D'après le Département de l'Énergie américain, la moitié de l'électricité utilisée par des centres de données en 2024 était déjà lié à l'intelligence artificielle. L'essentiel de cette contribution de l'IA est due à un groupe réduit de serveurs (moins de 10% en 2024) équipés de processeurs optimisés pour l'IA, les processeurs graphiques ou GPU (Graphics Processing Unit). 

>[!info] Des jeux vidéos à l'IA
>**Graphics Processing Unit**. Comme son nom l'indique, cette technologie servait auparavant à calculer l'affichage de pixels pour des graphismes, principalement de jeux vidéos, souvent considérés parmi les programmes les plus complexes et intensifs en ressources. 
>**Les GPUs permettent de traiter en parallèle de très nombreux calculs**, et ainsi dans le domaine de l'IA, d'accélérer considérablement la durée de l'entraînement d'un modèle.  
>
>L'utilisation de GPUs connait depuis 2017 essor sans précédent, qui fait en 2025 de **Nvidia** - _qui a un quasi monopole de la production de GPUs_ - **l'entreprise avec la plus forte cotation boursière dans le monde.** Ce dernier élément donne à lui seul une idée de la force de la croissance de cette technologie, Nvidia passant d'une cotation à 60 milliards de dollars début 2017, à 4 trillions de dollars en 2025[^1]. 

L'équipement massif des centres de données en GPUs traduit le passage d'un usage auparavant spécialisé essentiellement dans le stockage et la gestion de flux de données (liées à des sites web plus ou moins dynamiques), à l'ajout avec l'IA d'**un nouvel usage qui se concentre sur les calculs et intensifie les flux.** 
Ces nouveaux besoins en calculs demandent un équipement plus gourmand en énergie, l'augmentation de l'utilisation d'énergie produit plus de chaleur, elle-même entraînant d'autres dépenses d'énergie pour refroidir les équipements. **Cette série de conséquences change l'ensemble de l'infrastructure des centres de données qui accueillent des usages liés à l'IA.**

L'une des formes de cette transformation de l'infrastructure des centres de donnée est leur **changement d'échelle** progressif. Les acteurs les plus puissants de l'IA tels qu'OpenAI, Google, Meta, Microsoft possèdent leurs propres centres de données "_Hyperscale_" (à hyper-échelle) pour entraîner leurs modèles. Les entreprises de taille plus modeste souhaitant également entraîner ou déployer leurs outils d'IA se partagent des centres dits "_en colocation_" de large échelle. 
Ces deux types de centres de données précèdent l'avènement de l'IA générative mais passent d'une concentration de 20% de l'ensemble des serveurs américains en 2016, à un peu plus de 60% en 2024 ([ref](https://escholarship.org/uc/item/32d6m0d1#page=37)), avec une inflexion marquée de cette courbe précisément dès 2017. 
Du fait de leur dimensionnement et de la construction récente d'une large proportion d'entre eux, ces centres sont les plus efficaces en termes de consommation d'électricité[^6]. On peut cependant supposer que vu la très forte augmentation de l'usage d'énergie du secteur depuis 2017, ces améliorations ont davantage alimenté un effet rebond[^9], que limité la consommation.

L'impact de l'IA dans les centres de données a également pour effet d'augmenter leur **consommation d'eau douce, souvent potable**, notamment pour refroidir les GPUs à travers différentes solutions techniques. Les centres de données à hyper et large échelle concentrent à eux seuls 84% de la consommation directe d'eau de l'ensemble des centres en 2023, qui atteint la même année 66 milliards de litres ([ref](https://www.energy.gov/articles/doe-releases-new-report-evaluating-increase-electricity-demand-data-centers)). Ce chiffre est impressionnant, mais cette consommation _annuelle_ représente seulement 6% de la consommation totale d'eau douce américaine en un seul jour[^11].
**La principale cause de consommation d'eau douce par les centres de données se trouve en réalité dans leurs sources d'électricité**. Si on ajoute aux quantités précédentes - _directement consommées par les centres_ -, l'eau consommée par les différentes formes de centrales thermiques pour les alimenter, on atteint pour l'ensemble de l'année 2023 environ 860 milliards de litres d'eau consommés, soit 80% de la consommation totale d'eau douce américaine en un seul jour.
En combinant ces deux sources de consommation d'eau, on estime que l'entraînement d'un modèle de la taille de ChatGPT-3[^10]  consomme environ 5,5 millions de litres d'eau.

À l'échelle de l'ensemble des consommations américaines, les quantités d'électricité et encore plus d'eau utilisées peuvent sembler peu importantes. La vitesse de leur croissance pourrait cependant poser problème dans un avenir proche, si en 2023 les centres de données pesaient 4% dans la consommation électrique nationale, ils pourraient atteindre 12% en 2028 ([ref](https://www.energy.gov/articles/doe-releases-new-report-evaluating-increase-electricity-demand-data-centers)), soit un niveau proche de celui consommé actuellement pour le résidentiel (15%([ref](https://www.eia.gov/energyexplained/us-energy-facts/images/consumption-by-source-and-sector.pdf))). 

>[!info] En Europe, la dynamique inquiétante de l'Irlande
>Ce problème existe déjà en Irlande, qui - _après avoir mis en place une politique favorable à l'investissement des géants du numérique_ - consomme aujourd'hui 20% de son électricité pour les centres de données, avec des projections à 30% pour 2028, ce qui en ferait la première source de consommation d'électricité nationale ([ref](https://theshiftproject.org/app/uploads/2025/04/2025_03_06-TSP-Rapport-intermediaire-IA-quelles-infra-num-monde-decarbone.pdf)).